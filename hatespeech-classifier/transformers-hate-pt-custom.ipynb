{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effortless models deployment withÂ Mlflow\n",
    "## Fine tunning a text classification model based on BERT to detect hate speech on tweets in portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from jobtools.arguments import file2namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from hatedetection.text_datasets import ClassificationDataset\n",
    "from hatedetection.hate_detection_classifier import HateDetectionClassifier\n",
    "from hatedetection.evaluation import compute_classification_metrics\n",
    "from hatedetection.text_preparation import load_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load same data for using in our training. We have 2 datasets available, train and testing. As you can image, we will use train for training and eval for evaluation of the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset_path = 'data/train/*.csv'\n",
    "eval_dataset_path = 'data/eval/*.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_train, labels_train = load_examples(input_dataset_path)\n",
    "examples_eval, labels_eval = load_examples(eval_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure the experiment we will use with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"hate-pt-speech-mlflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our training configuration\n",
    "\n",
    "We will place all the configuration for our training routine in a `YAML` file. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = file2namespace('train.params.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The method `file2namespace` allows us to parse a `YAML` file into a `SimpleNamespace` object that we can manipulate easily to access the configuration of our model. It is always a good practice to take away hiperparamters from our training code.\n",
    "\n",
    "The file contents looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat train.params.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an instance of our HateDetectionClassifier. This class contains:\n",
    "\n",
    "- A loader function `_load_pyfunc` that allows us to register this model as a `pyfunc` model in MLFlow.\n",
    "- A method `predict()` that conforms the signature we will define later for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = HateDetectionClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fine tune this model to help us resolve the specific task in hand. The base model is a BERT architecture that has been trainined with text in portuguese. Our baseline is the model `neuralmind/bert-base-portuguese-cased` registered in HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.build(baseline=params.model.baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This method does the following:\n",
    "\n",
    "- It loads the tokenizer for the given pretrained model.\n",
    "- It loads the classifier for the given pretrained model. Since this base model has not been fine tuned to perform any specific task, we will recieve a warning about that. Don't worry, cause that's what we are looking for. We will fine tune this model with data specific to the task in hand.\n",
    "\n",
    "```python\n",
    "def build(self, baseline: str, tokenizer: str = None):\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer or baseline)\n",
    "    self.model = AutoModelForSequenceClassification.from_pretrained(baseline)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling long sequences of text\n",
    "\n",
    "Transfomers are well know for growing exponentially in memory requirements regarding the lenght of the sequences they process. Even though there are models that can take longer senquences that the one we are using, it will always be a limit because memory is not infinit. So we need a way to handle this. There are a couple of options here:\n",
    "\n",
    "- Truncate the sequences. Easy but data loss will occur.\n",
    "- Split long sequences in smaller sequences and then combine the results of the subsequences to make the final prediction.\n",
    "\n",
    "We are going to the the later here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.split_unique_words = params.data.preprocessing.split_unique_words\n",
    "classifier.split_seq_len = params.data.preprocessing.split_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reload the datasets with this parameters then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_train, labels_train = load_examples(input_dataset_path, \n",
    "                                             split_seq=True,\n",
    "                                             unique_words=params.data.preprocessing.split_unique_words,\n",
    "                                             seq_len = params.data.preprocessing.split_seq_len)\n",
    "examples_eval, labels_eval = load_examples(eval_dataset_path, \n",
    "                                           split_seq=True,\n",
    "                                           unique_words=params.data.preprocessing.split_unique_words,\n",
    "                                           seq_len = params.data.preprocessing.split_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build datasets to feed our transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassificationDataset(examples=examples_train,\n",
    "                                      labels=labels_train,\n",
    "                                      tokenizer=classifier.tokenizer)\n",
    "eval_dataset = ClassificationDataset(examples=examples_eval,\n",
    "                                     labels=labels_eval,\n",
    "                                     tokenizer=classifier.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(**vars(params.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=classifier.model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_classification_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_location=f\"{params.model.output_dir}/{params.model.name}\"\n",
    "artifacts = classifier.save_pretrained(saved_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import DataType\n",
    "\n",
    "input_schema = Schema([\n",
    "  ColSpec(DataType.string, \"text\"),\n",
    "])\n",
    "output_schema = Schema([\n",
    "  ColSpec(DataType.integer, \"hate\"),\n",
    "  ColSpec(DataType.double, \"confidence\"),\n",
    "])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_metrics(dict(filter(lambda item: item[1] is not None, evaluation_metrics.items())))\n",
    "mlflow.log_params(history.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pyfunc.log_model(\"classifier\", \n",
    "                        data_path=saved_location, \n",
    "                        code_path=[\"hatedetection\"], \n",
    "                        loader_module=\"hatedetection.hate_detection_classifier\", \n",
    "                        registered_model_name=\"hate-pt-speech\", \n",
    "                        signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
